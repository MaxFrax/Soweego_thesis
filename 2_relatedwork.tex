\clearpage
\newpage

\chapter{Related work}
\label{cha:2}
The alignment of Wikidata to third-party structured databases may tackle the lack of references, but it is a complex task. Although a given label can be the same between Wikidata and an external database, there could be ambiguous homonyms. To overcome the issue, we need to exploit other attributes in addition to labels. Choosing them is a challenge itself, since Wikidata and the target database have probably different attribute sets. It is not even assumable that the attributes will be the same among all the entities in the same KB, like Wikidata. 

\texttt{SocialLink} is a system that aligns KB entries of people and organizations to the  corresponding social media profiles,\cite{DBLP:conf/semweb/NechaevCG17a} and shares our problem. Its approach is to pick a minimal subset of attributes: name, difference between person and organization and temporal information that tells if the entity is alive or existent. Similarly, we choose full name, birth and death dates, as well as a set of URLs related to the entity. Unlike \texttt{SocialLink}, we allow the addition of target-specific attributes.

The exceeding attributes can improve the linking process, but they can also be exploited in a KB population task. In fact, mapping the semantics of these attributes against the Wikidata ontology would result in the addition of referenced statements. These statements cannot replace the work done by \texttt{StrepHit}\cite{DBLP:journals/semweb/FossatiDG18} or the one described in \cite{self:SocialLink/TypePrediction}, but we still view it as a contribution. In contrast to us, \texttt{StrepHit} focuses on unstructured data, typically free-text documents from Web sources.

\cite{self:SocialLink/Embeddings} exploits an enhanced representation of the social media content, compared to \cite{DBLP:conf/sac/NechaevCG17}. Despite the improvement, we argue that the approach will not be helpful in \texttt{soweego}, since we cannot assume the availability of any social media data.

The alignment task deals with a lot of queries on those attributes, so working with the target database APIs could be an issue: not only API usage is restricted, but Web requests also bring latency in code execution. Similarly to \cite{DBLP:conf/sac/NechaevCG17}, we work with an iternal database, but we populate it through the target dumps, instead of the social medium feed.

